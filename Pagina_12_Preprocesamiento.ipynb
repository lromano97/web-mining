{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Pagina_12_Preprocesamiento.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m9bAjWawC5UH"
      },
      "source": [
        "# Preprocesamiento"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PJJQDWrRC0Qj"
      },
      "source": [
        "# Instalacion de Bibliotecas\n",
        "!pip install gdown --quiet"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SFUizPRnBPe2"
      },
      "source": [
        "# Importo bibliotecas\n",
        "import gdown\n",
        "import sys\n",
        "import warnings\n",
        "import pprint\n",
        "import gc\n",
        "import pandas as pd\n",
        "import snowballstemmer\n",
        "\n",
        "from sklearn.externals import joblib\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# PrettyPrinter\n",
        "pp = pprint.PrettyPrinter(compact=True)\n",
        "pp = pprint.PrettyPrinter(indent=4, compact=True)\n",
        "\n",
        "# Libero memoria\n",
        "gc.collect()\n",
        "\n",
        "# No mostrar warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Limite de recursion\n",
        "sys.setrecursionlimit(30000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        },
        "id": "oxdDp-zxCOY0",
        "outputId": "ba8cead8-8757-4daa-aff7-79e0570aff18"
      },
      "source": [
        "# Descargo datasets de secciones de noticias\n",
        "gdown.download('https://drive.google.com/uc?id=1qLM1mV45A9-hUI9dWb4SwtqCmfmAA35R', 'Sociedad.sav', quiet=False)\n",
        "gdown.download('https://drive.google.com/uc?id=19KLD-8nRba8XWwcw6YjChwB4KSZ10g6v', 'Economia.sav', quiet=False)\n",
        "gdown.download('https://drive.google.com/uc?id=1juCpm4wAXHFXH3IBVQ3FrkfQ7I8r9BLn', 'ElMundo.sav', quiet=False)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1qLM1mV45A9-hUI9dWb4SwtqCmfmAA35R\n",
            "To: /content/Sociedad.sav\n",
            "824MB [00:04, 203MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=19KLD-8nRba8XWwcw6YjChwB4KSZ10g6v\n",
            "To: /content/Economia.sav\n",
            "839MB [00:25, 32.6MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1juCpm4wAXHFXH3IBVQ3FrkfQ7I8r9BLn\n",
            "To: /content/ElMundo.sav\n",
            "814MB [00:18, 43.9MB/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'ElMundo.sav'"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pbH5o_0nDcMk"
      },
      "source": [
        "# Cargo secciones (Aproximadamente toma 5 minutos en cargar las tres secciones)\n",
        "society_section = joblib.load('Sociedad.sav')\n",
        "economy_section = joblib.load('Economia.sav')\n",
        "world_section = joblib.load('ElMundo.sav')"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "40TWBidjHG1n"
      },
      "source": [
        "# Concatenamos secciones\n",
        "news = pd.concat([society_section, economy_section, world_section], axis=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "21yHygRIHH3s"
      },
      "source": [
        "# Borramos temporal de secciones\n",
        "del society_section\n",
        "del economy_section\n",
        "del world_section\n",
        "\n",
        "# Libero memoria\n",
        "gc.collect()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FFQu0839D9Bu"
      },
      "source": [
        "# Stopwords\n",
        "stopwords_es = pd.read_csv('https://drive.google.com/uc?export=download&id=1prdR9zCvSQnEIZoLf5B0TiS1tUhMLClC', header = None)\n",
        "stopwords_es_sin_acentos = pd.read_csv('https://drive.google.com/uc?export=download&id=1QDLZXPDnJ1XbHJRukDgqxi5P0AKtssd_', header = None)\n",
        "\n",
        "stopwords = pd.concat([stopwords_es, stopwords_es_sin_acentos])"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DDOJHqMiEc5T"
      },
      "source": [
        "def remove_stop_words(text):\n",
        "  \"\"\"\n",
        "    Remueve stop words en inglés\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    text: list\n",
        "      lista de palabras (tokens) a filtrar\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    list\n",
        "      lista de palabras sin los stop words\n",
        "  \"\"\"\n",
        "  return [token for token in text if token.lower() not in stopwords]"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QkkzUGExEdRy"
      },
      "source": [
        "def tokenize(text): \n",
        "  \"\"\"\n",
        "  :param text: Una expresion regular que define que es un token\n",
        "  :return: Una funcion que recibe un texto y retorna el texto tokenizado.\n",
        "  \"\"\"\n",
        "  if text is None:\n",
        "    text = r\"[a-zA-ZâáàãõáêéíóôõúüÁÉÍÓÚñÑçÇ][0-9a-zA-ZâáàãõáêéíóôõúüÁÉÍÓÚñÑçÇ]+\"\n",
        "  token_pattern = re.compile(text)\n",
        "  return lambda doc: token_pattern.findall(doc)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HIxt6J9MEdVu"
      },
      "source": [
        "stemmer = snowballstemmer.stemmer(\"spanish\")\n",
        "\n",
        "def stem_words(tokens):\n",
        "    \"\"\"\n",
        "    Transforma mediante un stemmer a una secuencia de tokens.\n",
        "    :param tokens: Una secuencia de tokens.\n",
        "    :return La secuencia de tokens transformada por el stemmer.\n",
        "    \"\"\"\n",
        "    global stemmer\n",
        "    return [stemmer.stem(word) for word in tokens]"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NtSPecIoEdYB"
      },
      "source": [
        "def clean_short_words(text):\n",
        "  \"\"\"\n",
        "    Limpia palabras con longitud 1\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    text: str\n",
        "      documento a tokenizar\n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "    list\n",
        "      lista de tokens\n",
        "  \"\"\"\n",
        "  return [word for word in text if len(word) > 1]"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dk0af5VZEdaf"
      },
      "source": [
        "\n",
        "def preprocess_text(text):\n",
        "  \"\"\"\n",
        "    Pre-procesamiento\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    text: str\n",
        "      documento a analizar\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    pd.DataFrame\n",
        "      retorna un dataframe con las 20 palabras que más se repiten y su frecuencia\n",
        "  \"\"\"\n",
        "  tokenized = tokenize(text)\n",
        "  without_stops = remove_stop_words(tokenized)\n",
        "  without_short_words = clean_short_words(without_stops)\n",
        "  stemmed_words = stem_words(without_short_words)\n",
        "  return stemmed_words"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wVysaHReEdcw"
      },
      "source": [
        "# cantidad minima de docs que tienen que tener a un token para conservarlo.\n",
        "MIN_DF=3\n",
        "# cantidad maxima de docs que tienen que tener a un token para conservarlo.\n",
        "MAX_DF=0.8\n",
        "# numero minimo tokens consecutivos que se consideran\n",
        "MIN_NGRAMS=1\n",
        "# numero maximo tokens consecutivos que se consideran\n",
        "MAX_NGRAMS=2\n",
        "\n",
        "# aplicamos count vectorizer\n",
        "vectorizer = CountVectorizer(tokenizer=preprocess_text,\n",
        "                                 lowercase=True, strip_accents='unicode', decode_error='ignore',\n",
        "                                 ngram_range=(MIN_NGRAMS, MAX_NGRAMS), min_df=MIN_DF, max_df=MAX_DF)\n",
        "\n",
        "#vectorizer.fit_transform(DATASETFALOPA)"
      ],
      "execution_count": 14,
      "outputs": []
    }
  ]
}