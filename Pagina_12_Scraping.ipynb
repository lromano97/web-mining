{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Pagina_12_Scraping.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "zbx-OFj1r436"
      },
      "source": [
        "# Instalo bibliotecas\n",
        "!pip install bs4 --quiet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rb4OaI2fq1ky"
      },
      "source": [
        "# Importo\n",
        "import pandas as pd\n",
        "import re \n",
        "import gc\n",
        "import requests\n",
        "import time\n",
        "import sys\n",
        "import warnings\n",
        "import pprint\n",
        "import concurrent.futures\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "from sklearn.externals import joblib\n",
        "\n",
        "# PrettyPrinter\n",
        "pp = pprint.PrettyPrinter(compact=True)\n",
        "pp = pprint.PrettyPrinter(indent=4, compact=True)\n",
        "\n",
        "# Libero memoria\n",
        "gc.collect()\n",
        "\n",
        "# No mostrar warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Limite de recursion\n",
        "sys.setrecursionlimit(30000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VnuBQ2y3aXx9"
      },
      "source": [
        "# Inicializacion de constantes\n",
        "\n",
        "# Defino cantidad de topicos y paginas\n",
        "topics = ['economia', 'el-mundo', 'sociedad']\n",
        "pages = range(1, 750, 5)\n",
        "\n",
        "# Defino headers del request\n",
        "request_headers = {\n",
        "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/93.0.4577.82 Safari/537.36',\n",
        "    'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CvZOIdZ4r-F6"
      },
      "source": [
        "# Creo funcion que retorna las noticias de un topico determinado\n",
        "RegExp = re.compile('\\\\n*')\n",
        "\n",
        "# Defino pagina base\n",
        "base_page = 'https://www.pagina12.com.ar'\n",
        "\n",
        "def retrieve_topic_news(topic):\n",
        "  topic_news = []\n",
        "  for page in pages:\n",
        "    request = requests.get(f'{base_page}/secciones/{topic}?page={page}', headers=request_headers, timeout=None)\n",
        "    soup = BeautifulSoup(request.text, 'html.parser')\n",
        "    articles = soup.find_all('article', class_='article-item article-item--teaser ')\n",
        "    for article in articles:\n",
        "      anchor = article.find('a', class_='p12-separator--left--primary')\n",
        "      if anchor is not None:\n",
        "        article_detail = requests.get(f'{base_page}/{anchor[\"href\"]}', headers=request_headers, timeout=None)\n",
        "        soup_detail = BeautifulSoup(article_detail.text, 'html.parser')\n",
        "        paragraphs_div = soup_detail.find('div', class_='article-main-content article-text ')\n",
        "        dates = soup_detail.find('div',class_='article-info')\n",
        "        if paragraphs_div is not None and dates is not None:\n",
        "          dates = dates.find('span').string\n",
        "          paragraphs = paragraphs_div.find_all('p')\n",
        "          news = ''\n",
        "          for paragraph in paragraphs:\n",
        "            if paragraph.string is not None:\n",
        "              news += paragraph.string\n",
        "          if news != '' and not RegExp.fullmatch(news):\n",
        "            topic_news.append({'url':f'{base_page}/{anchor[\"href\"]}','paragraph':news,'date':dates,'topic': topic})\n",
        "    time.sleep(3)\n",
        "  return pd.DataFrame(topic_news)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yc4bBRAqaV7q"
      },
      "source": [
        "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
        "  topics_results = executor.map(retrieve_topic_news, topics)\n",
        "  topics_news = pd.DataFrame()\n",
        "  for topic_result in topics_results:\n",
        "    topics_news = pd.concat([topics_news, topic_result])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x8QOjrrVWcmu"
      },
      "source": [
        "del topics_results\n",
        "gc.collect()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PwKjtE1TXKcs"
      },
      "source": [
        "# Exportamos informacion adquirida del web scraping\n",
        "joblib.dump(topics_news, 'news.sav')  "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}