{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Pagina_12_Scraping.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "zbx-OFj1r436"
      },
      "source": [
        "# Instalo bibliotecas\n",
        "!pip install bs4 --quiet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rb4OaI2fq1ky"
      },
      "source": [
        "# Importo\n",
        "import pandas as pd\n",
        "import pickle \n",
        "import re \n",
        "import requests\n",
        "import time\n",
        "import sys\n",
        "import warnings\n",
        "import pprint\n",
        "import concurrent.futures\n",
        "import snowballstemmer\n",
        "\n",
        "from google.colab import files\n",
        "from bs4 import BeautifulSoup\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# PrettyPrinter\n",
        "pp = pprint.PrettyPrinter(compact=True)\n",
        "pp = pprint.PrettyPrinter(indent=4, compact=True)\n",
        "\n",
        "# No mostrar warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VnuBQ2y3aXx9"
      },
      "source": [
        "# Inicializacion de constantes\n",
        "\n",
        "# Defino cantidad de topicos y paginas\n",
        "topics = ['economia', 'el-mundo', 'sociedad']\n",
        "pages = range(1, 1000)\n",
        "\n",
        "# Defino headers del request\n",
        "request_headers = {\n",
        "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/68.0.3440.84 Safari/537.36',\n",
        "    'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CvZOIdZ4r-F6"
      },
      "source": [
        "# Creo funcion que retorna las noticias de un topico determinado\n",
        "RegExp = re.compile('\\\\n*')\n",
        "\n",
        "def retrieve_topic_news(topic):\n",
        "  topic_news = []\n",
        "  for page in pages:\n",
        "    request = requests.get(f'https://www.pagina12.com.ar/secciones/{topic}?page={page}', headers=request_headers)\n",
        "    soup = BeautifulSoup(request.text, 'html.parser')\n",
        "    articles = soup.find_all('article', class_='article-item article-item--teaser ')\n",
        "    for article in articles:\n",
        "      anchor = article.find('a', class_='p12-separator--left--primary')\n",
        "      if anchor is not None:\n",
        "        article_detail = requests.get(f'https://www.pagina12.com.ar/{anchor[\"href\"]}', headers=request_headers)\n",
        "        soup_detail = BeautifulSoup(article_detail.text, 'html.parser')\n",
        "        paragraphs_div = soup_detail.find('div', class_='article-main-content article-text ')\n",
        "        if paragraphs_div is not None:\n",
        "          dates = soup_detail.find('div',class_='article-info')\n",
        "          if dates is not None:\n",
        "            dates = dates.find('span').string\n",
        "            paragraphs = paragraphs_div.find_all('p')\n",
        "            news = ''\n",
        "            for paragraph in paragraphs:\n",
        "              if paragraph.string is not None:\n",
        "                news += paragraph.string\n",
        "            if news != '' and not RegExp.fullmatch(news):\n",
        "              topic_news.append({'url':f'https://www.pagina12.com.ar/{anchor[\"href\"]}','paragraph':news,'date':dates,'topic': topic})\n",
        "    time.sleep(3)\n",
        "  return pd.DataFrame(topic_news)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yc4bBRAqaV7q"
      },
      "source": [
        "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
        "  topics_results = executor.map(retrieve_topic_news, topics)\n",
        "  topics_news = pd.DataFrame()\n",
        "  for topic_result in topics_results:\n",
        "    topics_news = pd.concat([topics_news, topic_result])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QJ8p39OZV0DK"
      },
      "source": [
        "sys.setrecursionlimit(10000)\n",
        "\n",
        "with open('TP1.pickle', 'wb') as handle:\n",
        "    pickle.dump(topics_news, handle, protocol=pickle.HIGHEST_PROTOCOL)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LlFuia7lZA_l"
      },
      "source": [
        "files.download('TP1.pickle')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xl1jVEp1s0VD"
      },
      "source": [
        "with open('TP1.pickle', 'rb') as handle:\n",
        "    print(pickle.load(handle))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vSt6GIir9F8-"
      },
      "source": [
        "# Stopwords\n",
        "stopwords_es = pd.read_csv('/content/stopwords_es.txt', header = None)\n",
        "stopwords_es_sin_acentos = pd.read_csv('/content/stopwords_es_sin_acentos.txt', header = None)\n",
        "\n",
        "stopwords = pd.concat([stopwords_es, stopwords_es_sin_acentos])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ysxYkuBY9x0t"
      },
      "source": [
        "def remove_stop_words(text):\n",
        "  \"\"\"\n",
        "    Remueve stop words en inglés\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    text: list\n",
        "      lista de palabras (tokens) a filtrar\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    list\n",
        "      lista de palabras sin los stop words\n",
        "  \"\"\"\n",
        "  return [token for token in text if token.lower() not in stopwords]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W4l0OHJ67RDM"
      },
      "source": [
        "def tokenize(text): \n",
        "  \"\"\"\n",
        "  :param text: Una expresion regular que define que es un token\n",
        "  :return: Una funcion que recibe un texto y retorna el texto tokenizado.\n",
        "  \"\"\"\n",
        "  if text is None:\n",
        "    text = r\"[a-zA-ZâáàãõáêéíóôõúüÁÉÍÓÚñÑçÇ][0-9a-zA-ZâáàãõáêéíóôõúüÁÉÍÓÚñÑçÇ]+\"\n",
        "  token_pattern = re.compile(text)\n",
        "  return lambda doc: token_pattern.findall(doc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MFDa8Ok97Yoi"
      },
      "source": [
        "stemmer = snowballstemmer.stemmer(\"spanish\")\n",
        "\n",
        "def stem_words(tokens):\n",
        "    \"\"\"\n",
        "    Transforma mediante un stemmer a una secuencia de tokens.\n",
        "    :param tokens: Una secuencia de tokens.\n",
        "    :return La secuencia de tokens transformada por el stemmer.\n",
        "    \"\"\"\n",
        "    global stemmer\n",
        "    return [stemmer.stem(word) for word in tokens]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u-uh5Tb58TD8"
      },
      "source": [
        "def clean_short_words(text):\n",
        "  \"\"\"\n",
        "    Limpia palabras con longitud 1\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    text: str\n",
        "      documento a tokenizar\n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "    list\n",
        "      lista de tokens\n",
        "  \"\"\"\n",
        "  return [word for word in text if len(word) > 1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y5HsakToD1GO"
      },
      "source": [
        "def preprocess_text(text):\n",
        "  \"\"\"\n",
        "    Pre-procesamiento\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    text: str\n",
        "      documento a analizar\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    pd.DataFrame\n",
        "      retorna un dataframe con las 20 palabras que más se repiten y su frecuencia\n",
        "  \"\"\"\n",
        "  tokenized = tokenize(text)\n",
        "  without_stops = remove_stop_words(tokenized)\n",
        "  without_short_words = clean_short_words(without_stops)\n",
        "  stemmed_words = stem_words(without_short_words)\n",
        "  return stemmed_words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PT1UmOEh-64N"
      },
      "source": [
        "# cantidad minima de docs que tienen que tener a un token para conservarlo.\n",
        "MIN_DF=3\n",
        "# cantidad maxima de docs que tienen que tener a un token para conservarlo.\n",
        "MAX_DF=0.8\n",
        "# numero minimo tokens consecutivos que se consideran\n",
        "MIN_NGRAMS=1\n",
        "# numero maximo tokens consecutivos que se consideran\n",
        "MAX_NGRAMS=2\n",
        "\n",
        "# aplicamos count vectorizer\n",
        "vectorizer = CountVectorizer(tokenizer=preprocess_text,\n",
        "                                 lowercase=True, strip_accents='unicode', decode_error='ignore',\n",
        "                                 ngram_range=(MIN_NGRAMS, MAX_NGRAMS), min_df=MIN_DF, max_df=MAX_DF)\n",
        "\n",
        "#vectorizer.fit_transform(DATASETFALOPA)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}